---
title: "Analysis of congressional speeches"
output: github_document
---


```{r setup, include=FALSE, echo = FALSE}
# Basic knitr options
library(knitr)
opts_chunk$set(comment = NA, 
               echo = FALSE, 
               warning = FALSE, 
               message = FALSE, 
               error = TRUE, 
               cache = FALSE,
               fig.path = 'figures/')
```


```{r}
# Libraries
library(vilaweb)
library(rtweet)
library(tidyverse)
library(databrew)
library(waffle)
library(stringr)
library(tidyverse)
library(languageR) #https://www.rdocumentation.org/packages/languageR/versions/1.4.1/topics/compare.richness.fnc
library(quanteda) # https://www.rdocumentation.org/packages/quanteda/versions/0.9.8.5/topics/lexdiv
library(readr)
library(dplyr)
library(stringr)
library(lubridate)
library(tidyr)
library(tidytext)
library(quanteda)
library(ggplot2)
library(DT)
```

```{r}
# Define some functions
simplify_text <- function(x){
  x <- gsub("[\r\n]", "", x)
  x <- str_replace(gsub("\\s+", " ", str_trim(x)), "B", "b")
  return(x)
}

clean_text <- function(x){
  x <- str_replace_all(x, "[[:punct:]]", " ")
  x <- tolower(x)
  x <- simplify_text(x)
  return(x)
}

make_word_vector <- function(x){
  x <- clean_text(x)
  x_parsed <- strsplit(x, " ")
  x <- unlist(x_parsed)
}

make_word_df <- function(x){
  x <- make_word_vector(x)
  x <- data_frame(word = x)
  x <- x %>% group_by(word) %>% summarise(freq = n()) %>% arrange(desc(freq)) %>%
    mutate(cs = cumsum(freq)) %>%
    mutate(p = freq / sum(freq)) %>%
    mutate(psc = cs / sum(freq)) %>%
    filter(! word %in% c('', '\n'))
  # Calculate type token ratio
  # (types = total number of DIFFERENT words)
  # (tokens = total number of words)
  return(x)
}

# Define function for stemming
get_stem <- function(x, lang = NULL){
  stemmer <- vilaweb::stem
  if(!is.null(lang)){
    stemmer <- stemmer %>% filter(language == lang)
  } else {
    message('No lang provided. Will scan all languages.')
  }
  
# Keep only the word in question
  find_stem <- function(y){
    out <- stemmer %>%
    filter(tolower(y) == original) %>%
      .$stem
    if(length(out) == 0){
      out <- y
    }
    if(length(out) > 1){
      out <- out[1]
    }
    return(out)
  }
  
  # Loop through each element of the vector
  out_list <- list()
  for(i in 1:length(x)){
    message('Finding stems for ', i , ' of ', length(x))
    # Get separated by spaces
    x_parsed <- unlist(strsplit(x[i], ' '))
    x_done <- unlist(lapply(x_parsed, find_stem))
    x_done <- paste0(x_done, collapse = ' ')
    out_list[[i]] <- x_done
  }
  out <- unlist(out_list)
  return(out)
}


# Make a score sentiment function
score_sentiment <- function (x, language = 'es',
                             valence_only = TRUE) {
  # Get the afinn and nrc dictionary
    af <- vilaweb::afinn
    nr <- vilaweb::nrc

    # Define the column for the right language
    word <- af %>% dplyr::select_(language)
    names(word) <- 'word'
    af$word <- word$word
    # Define the nr data for the right language
    if(language == 'en'){
      nr <- nrc %>% filter(lang == 'english')
    }
    if(language == 'ca'){
      nr <- nrc %>% filter(lang == 'catalan')
    }
    if(language == 'es'){
      nr <- nrc %>% filter(lang == 'spanish')
    }
    
    x <- clean_text(x)
    # Get stems
    message('Finding stems')
    x <- get_stem(x, lang = language)
    # Split at spaces
    x_parsed <- strsplit(x, " ")
    out <- rep(NA, length(x))
    nr_out <- list()
    for (i in 1:length(out)) {
      message('Scoring sentiment for ', i, ' of ', length(out))
        this_element <- x[i]
        this_element_parsed <- x_parsed[[i]]
        # Get af just for this
        af_small <- af %>% filter(word %in% this_element_parsed)
        if (nrow(af_small) == 0) {
            out[i] <- 0
        }
        else {
            out[i] <- mean(af_small$score, na.rm = TRUE)
        }
        
        # Get nr just for this
        nr_small <- nr %>% filter(word %in% this_element_parsed)
        nrc_names <- sort(unique(nrc$sentiment))
        nr_df <- data.frame(matrix(rep(0, length(nrc_names)), nrow = 1))
        names(nr_df) <- nrc_names
        if(nrow(nr_small) > 0){
          nr_scored <- nr_small %>%
            group_by(sentiment) %>%
            summarise(value = sum(value)) %>%
            ungroup
          for(j in 1:nrow(nr_scored)){
            nr_df[,nr_scored$sentiment[j]] <- nr_scored$value[j]
          }
        } 
        nr_out[[i]] <- nr_df
    }
    nr_out <- bind_rows(nr_out)
    nr_out$sentiment <- out
    if(valence_only){
      return(out)
    } else {
      return(nr_out)
    }
}
```

```{r}
# Read in stopwords
catalan_spanish_stopwords <- 
  c(
    readLines('stopwords/stopwords-ca.txt'),
    readLines('stopwords/stopwords-es.txt')
  )
# Add a few
catalan_spanish_stopwords <- 
  c(catalan_spanish_stopwords,
    c('señor',
      'señoría',
      'señorías',
      'sánchez'))
# Remove numbers
numbers <- as.character(c(0:154,156:1935,1937:1977, 1979:2018))
catalan_spanish_stopwords <- c(catalan_spanish_stopwords, numbers)

file_name <- 'processed_transcript.RData'
if(!file_name %in% dir()){
    # Read in transcripts from session de control
  transcript <- read_csv('data/transcript.csv') %>%
    dplyr::select(date, source, person, text, qa) %>%
    # Remove qa
    filter(!qa) %>%
    # Keep only relevant speakers
    filter(person %in% c('Pedro Sánchez',
                  'Pablo Casado',
                  'Albert Rivera',
                  'Pablo Iglesias',
                  'Joan Tardà',
                  'Carles Campuzano')) %>%
    # remove certain terms
    mutate(text = gsub('(Aplausos)', '', text, fixed = TRUE)) %>%
    mutate(text = gsub('(Rumores)', '', text, fixed = TRUE)) %>%
    mutate(text = gsub('(Risas)', '', text, fixed = TRUE)) %>%
    mutate(text = gsub('(Protestas)', '', text, fixed = TRUE))
  
  # Get by sentence
  sentencify <- function(transcript){
    # Define whether there was an interruption
    transcript$interruption <- FALSE
    for(i in 2:nrow(transcript)){
      if(transcript$person[i] != transcript$person[i-1]){
        transcript$interruption[i] <- TRUE
      }
    }
    # Use interruptions to get intervention number
    transcript$intervention_number <- NA
    counter <- 1
    for(i in 1:nrow(transcript)){
      if(transcript$interruption[i]){
        counter <- counter + 1
      }
      transcript$intervention_number[i] <- counter
    }
    out_list <- list()
    for(i in 1:nrow(transcript)){
      sub_transcript <- transcript[i,]
      # split by sentence 
      sub_transcript_split <- unlist(strsplit(sub_transcript$text, split = '.', fixed = TRUE))
      sub_transcript_split <- trimws(sub_transcript_split)
      # If greater than 1, larger dataframe
      if(length(sub_transcript_split) > 1){
        out <- data_frame(date = sub_transcript$date,
                          source = sub_transcript$source,
                          person = sub_transcript$person,
                          text = sub_transcript_split,
                          intervention_number = sub_transcript$intervention_number)
      } else {
        out <- sub_transcript
      }
      out_list[[i]] <- out
    }
    out <- bind_rows(out_list)
    # Create a sentence number
    out <- out %>%
      mutate(cs = 1) %>%
      group_by(person) %>%
      mutate(sentence_number = cumsum(cs)) %>%
      ungroup %>%
      dplyr::select(-cs) %>%
      # Create a sentence %
      group_by(person) %>%
      mutate(sentence_percent = sentence_number / max(sentence_number) * 100) %>%
      ungroup
    return(out)
  }
  
  # Make transcript a 1 row per person-sentence df
  transcript <- sentencify(transcript = transcript)
  
  # Score the sentiment
  right <- score_sentiment(transcript$text, language = 'es', valence_only = FALSE)
  transcript <- bind_cols(transcript, right)
  save(transcript, file = file_name)
} else {
  load(file_name)
}


# Cumulative average polarity
ma <- function(arr, n=15){
  res = arr
  for(i in n:length(arr)){
    res[i] = mean(arr[(i-n):i])
  }
  res
}
transcript <- transcript %>%
  group_by(person, intervention_number) %>%
  mutate(sentiment_cumulative_average = ma(sentiment, 10)) %>%
  ungroup

# Flag words
flag_words <- function(x, 
                       words = c('generalitat', 'catalu', 'govern',
                   'catala', 'torra', 'independe',
                   'secioni', 'separat', 'barcelo', 
                   'carreter')){
  out <- list()
  for(i in 1:length(words)){
    out[[i]] <- grepl(words[i], tolower(x))
  }
  z <- data.frame(out, fix.empty.names = FALSE)
  names(z) <- words
  z <- as.matrix(z)
  z <- apply(z, 1, function(x){any(x)})
  return(z)
}

# Define word groups
catalan_words <- c('generalitat', 'catalu',
                   'catala', 'torra', 'independe',
                   'secioni', 'separat', 'barcelo')
violence_words <- 
  c('guerra', 'violen', 'balas', 'bala ', 'odio', 'sufrir', 'golp', 'tanqu', 
    'kale borroka', 'batasun', ' atac', ' ataque', 'muerto', 'morir', 'escrach', 'terror',
    'rebel')
spain_words <- c('constituc', 'españ')


# Identify catalan sentences in the data
transcript$catalan <- flag_words(transcript$text, words = catalan_words)
transcript$spanish <- flag_words(transcript$text, words = spain_words)
transcript$violence <- flag_words(transcript$text, words = violence_words)

# Combine
combined_df <- transcript %>%
  group_by(person, intervention_number) %>%
  summarise(text = paste0(text, collapse = ' ')) %>%
  ungroup

# Flatten (just one thing per person)
flattened_df <- 
  transcript %>%
  group_by(person) %>%
  summarise(text = paste0(text, collapse = ' ')) %>%
  ungroup


# PAIR WORDS
 # break text into bigrams
bigrams <- transcript %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, into = c("first","second"), sep = " ", remove = FALSE) %>%
  # # remove stop words from tidytext package 
  # anti_join(stop_words, by = c("first" = "word")) %>%
  # anti_join(stop_words, by = c("second" = "word")) %>%
  filter(!first %in% catalan_spanish_stopwords,
         !second %in% catalan_spanish_stopwords) %>%
  filter(str_detect(first, "[a-z]"),
         str_detect(second, "[a-z]")) %>%
  group_by(person) %>%
  count(bigram) %>%
  arrange(-n)

bigram_freqs <- bigrams %>% 
  left_join(bigrams %>% 
              group_by(person) %>% 
              summarise(total = sum(n))) %>%
  mutate(percent = n/total*100) %>%
  group_by(person)

# get the top bigram for each address
top_bigrams <- bigram_freqs %>%
  top_n(10) %>%
  arrange(-percent)

top_bigram_freqs <- bigram_freqs %>%
  semi_join(top_bigrams) %>%
  ungroup() %>%
  arrange(-percent) 
```

## Catalonia, Brexit, and Violence: An analysis of Pedro Sánchez's December 2018 speech to Congress (and reactions to it)


```{r}
corpus_x <- corpus(simplify_text(flattened_df$text),
                   docnames = flattened_df$person)
docvars(corpus_x, 'Person') <- flattened_df$person
# summary(corpus_x)
# texts(corpus_x)[1]
## Quick search
# kwic(corpus_x, "comandos")
# head(docvars(corpus_x))
## Extract tokens
# tokens(corpus_x, remove_numbers = TRUE,  remove_punct = TRUE)
## Document feature matrix

my_dfm <- dfm(corpus_x,
              remove_punct = TRUE,
              remove = catalan_spanish_stopwords)
# ## See top 20 words
# topfeatures(my_dfm, 20)
## Wordcloud
cols <- c(databrew::make_colors(n = length(unique(dim(my_dfm)[1]))))
textplot_wordcloud(my_dfm, min_count = 1, random_order = FALSE,
                   rotation = 0,#0.25, 
                   min_size = 0.65,
                   max_size = 2.8,
                   max_words = 1000,
                   # labelcolor = cols,
                   # labeloffset = 1,
                   labelsize = 1.2,
                   color = cols,
                   comparison = TRUE)

```

On Wednesday, December 12th, Spanish President Pedro Sánchez delivered an address to the Congreso de los Diputados regarding Brexit and the political situation in Catalonia ([official transcription here](http://www.congreso.es/public_oficiales/L12/CONG/DS/PL/DSCD-12-PL-170.PDF)). The speech reflected rising tensions between pro-independence Catalans and the pro-union Sánchez government, and marked a sharp break with Sánchez's previous more conciliatory tone towards Catalonia. The following back-and-forth between Sánchez and the leaders of other major Spanish political parties was tense, and marked by repeated references to violence.

What follows is linguistic analysis of the speeches and counter-speeches of 6 politicians:

- Pedro Sánchez (President, PSOE, unionist)
- Pablo Casado (PP, unionist)
- Albert Rivera (Ciudadanos, unionist)
- Pablo Iglesias (Podemos, ambivalent)
- Carles Campuzano (PDeCat, independentist)
- Joan Tardà (Catalan Left, independentist)

## The questions

1. Are there differences in "polarity" (postivity-negativity) between the different politicians' speeches?

2. Are there differences in the frequency of violence-associated words between the different politicians' speeches?

3. What is the relationship between emotional polarity, violence and references to Catalonia?

## The methods

We digitized the speeches from December 12 into a [machine-readable format](https://github.com/joebrew/vilaweb/blob/master/inst/rmd/sesion_de_control/data/transcript.csv), and then used an algorithm based on the [AFINN library](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010) (a dictionary of words with assigned sentimental polarity) to classify each sentence's average emotional direction. Certain words are categorized as positive or negative, with -5 being the most negative (for example, "bastard", "slut") and +5 being the most positive (for example, "superb" (magnífico) or "thrilled" (encantado)). The majority of words do not have an emotional weight ("to act", "administration", etc.) and are classified as 0. The average of a sentence's emotionally-weighted words constitute its positivity.

The below is an example of how the algorithm works on an ctual sentene from the speech. The sentence contained some negative words and some positive words, and was classified as neutral.

<table style="width:100%">
  <tr>
    <td><img src="img/sanchez2.png" /></td>
  </tr>
</table>

```{r}
# example <- c("Y lealtad entre administraciones, señorías, algo que por desgracia no se construye con declaraciones que se sitúan fuera de toda lógica y apelan a la violencia, como hemos escuchado en algún dirigente de la Generalitat de Cataluña. Que conste en acta por tanto: tiempo, diálogo y lealtad.")
# example_split <- unlist(strsplit(clean_text(example), ' '))
# 
# x <- score_sentiment(example_split, language = 'es',valence_only = TRUE)
# data.frame(example_split, x)
```

We ran the algorithm on the entire content of speeches, and analyzed trends in positivity. We also tabulated word frequencies and associations.


## The results

### Overall positivity

```{r}
plot_data <-
  transcript %>%
  group_by(person) %>%
  summarise(sentiment = mean(sentiment)) %>%
  ungroup

ggplot(data = plot_data,
       aes(x = person,
           y = sentiment)) +
  geom_bar(stat = 'identity')
```

### Frequency of violent words

```{r}
plot_data <-
  transcript %>%
  filter(intervention_number != 1) %>%
  group_by(person, sentence_percent) %>%
  summarise(viol = length(which(violence)) / n() * 100) %>%
  ungroup
ggplot(data = plot_data,
       aes(x = sentence_percent,
           y = viol)) +
  geom_point() +
  facet_wrap(~person, scales = 'free_x') +
  geom_smooth()
```

### Positivity over course of speech

```{r}
plot_data <- transcript %>% group_by(person) %>% filter(intervention_number == min(intervention_number)) %>% ungroup
ggplot(data = plot_data,
       aes(x = sentence_percent,
           y = sentiment)) +
  geom_point(size = 0.2) +
  facet_wrap(~person, scales = 'free_x') +
  geom_smooth()
```

# Lexical diversity

```{r}
# Lexical diversity
ld <- textstat_lexdiv(my_dfm,
                      # measure = c('R', 'S'),
                      measure = c("all"),
                      log.base = 10) %>%
  mutate(person = document) 
ld <- ld %>%
  gather(key, value, R:S) 
ggplot(data = ld,
       aes(x = person,
           y = value)) +
  geom_bar(stat = 'identity',
           position = position_dodge(width = 0.8)) +
  geom_text(size = 3,
            aes(label = round(value, digits = 3))) +
  facet_wrap(~key,
             scales= 'free_y') +
  theme(axis.text.x = element_text(angle = 90,
                                   vjust = 0.5,
                                   hjust = 1))

ggplot(data = transcript,
       aes(x = sentence_number,
           y = sentiment_cumulative_average)) +
  geom_point(data = transcript,
       aes(x = sentence_number,
           y = sentiment)) +
  geom_line() +
  facet_wrap(~person, scales = 'free') 

# Sentences which contain cataluna, polarity
transcript %>%
  group_by(catalan) %>%
  summarise(polarity= mean(sentiment),
            viol = length(which(violence)) / n())
transcript %>%
  group_by(spanish) %>%
  summarise(polarity= mean(sentiment))


```
