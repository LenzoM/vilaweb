---
title: "Analysis of congressional speeches"
output: github_document
---


```{r setup, include=FALSE, echo = FALSE}
# Basic knitr options
library(knitr)
opts_chunk$set(comment = NA, 
               echo = FALSE, 
               warning = FALSE, 
               message = FALSE, 
               error = TRUE, 
               cache = FALSE,
               fig.path = 'figures/')
```


```{r}
# Libraries
library(vilaweb)
library(rtweet)
library(tidyverse)
library(databrew)
library(waffle)
library(stringr)
library(tidyverse)
library(languageR) #https://www.rdocumentation.org/packages/languageR/versions/1.4.1/topics/compare.richness.fnc
library(quanteda) # https://www.rdocumentation.org/packages/quanteda/versions/0.9.8.5/topics/lexdiv
library(readr)
library(dplyr)
library(stringr)
library(lubridate)
library(tidyr)
library(tidytext)
library(quanteda)
library(ggplot2)
library(DT)
```

```{r}
# Define some functions
simplify_text <- function(x){
  x <- gsub("[\r\n]", "", x)
  x <- str_replace(gsub("\\s+", " ", str_trim(x)), "B", "b")
  return(x)
}

clean_text <- function(x){
  x <- str_replace_all(x, "[[:punct:]]", " ")
  x <- tolower(x)
  x <- simplify_text(x)
  return(x)
}

make_word_vector <- function(x){
  x <- clean_text(x)
  x_parsed <- strsplit(x, " ")
  x <- unlist(x_parsed)
}

make_word_df <- function(x){
  x <- make_word_vector(x)
  x <- data_frame(word = x)
  x <- x %>% group_by(word) %>% summarise(freq = n()) %>% arrange(desc(freq)) %>%
    mutate(cs = cumsum(freq)) %>%
    mutate(p = freq / sum(freq)) %>%
    mutate(psc = cs / sum(freq)) %>%
    filter(! word %in% c('', '\n'))
  # Calculate type token ratio
  # (types = total number of DIFFERENT words)
  # (tokens = total number of words)
  return(x)
}

# Define function for stemming
get_stem <- function(x, lang = NULL){
  stemmer <- vilaweb::stem
  if(!is.null(lang)){
    stemmer <- stemmer %>% filter(language == lang)
  } else {
    message('No lang provided. Will scan all languages.')
  }
  
# Keep only the word in question
  find_stem <- function(y){
    out <- stemmer %>%
    filter(tolower(y) == original) %>%
      .$stem
    if(length(out) == 0){
      out <- y
    }
    if(length(out) > 1){
      out <- out[1]
    }
    return(out)
  }
  
  # Loop through each element of the vector
  out_list <- list()
  for(i in 1:length(x)){
    message('Finding stems for ', i , ' of ', length(x))
    # Get separated by spaces
    x_parsed <- unlist(strsplit(x[i], ' '))
    x_done <- unlist(lapply(x_parsed, find_stem))
    x_done <- paste0(x_done, collapse = ' ')
    out_list[[i]] <- x_done
  }
  out <- unlist(out_list)
  return(out)
}


# Make a score sentiment function
score_sentiment <- function (x, language = 'es',
                             valence_only = TRUE) {
  # Get the afinn and nrc dictionary
    af <- vilaweb::afinn
    nr <- vilaweb::nrc

    # Define the column for the right language
    word <- af %>% dplyr::select_(language)
    names(word) <- 'word'
    af$word <- word$word
    # Define the nr data for the right language
    if(language == 'en'){
      nr <- nrc %>% filter(lang == 'english')
    }
    if(language == 'ca'){
      nr <- nrc %>% filter(lang == 'catalan')
    }
    if(language == 'es'){
      nr <- nrc %>% filter(lang == 'spanish')
    }
    
    x <- clean_text(x)
    # Get stems
    message('Finding stems')
    x <- get_stem(x, lang = language)
    # Split at spaces
    x_parsed <- strsplit(x, " ")
    out <- rep(NA, length(x))
    nr_out <- list()
    for (i in 1:length(out)) {
      message('Scoring sentiment for ', i, ' of ', length(out))
        this_element <- x[i]
        this_element_parsed <- x_parsed[[i]]
        # Get af just for this
        af_small <- af %>% filter(word %in% this_element_parsed)
        if (nrow(af_small) == 0) {
            out[i] <- 0
        }
        else {
            out[i] <- mean(af_small$score, na.rm = TRUE)
        }
        
        # Get nr just for this
        nr_small <- nr %>% filter(word %in% this_element_parsed)
        nrc_names <- sort(unique(nrc$sentiment))
        nr_df <- data.frame(matrix(rep(0, length(nrc_names)), nrow = 1))
        names(nr_df) <- nrc_names
        if(nrow(nr_small) > 0){
          nr_scored <- nr_small %>%
            group_by(sentiment) %>%
            summarise(value = sum(value)) %>%
            ungroup
          for(j in 1:nrow(nr_scored)){
            nr_df[,nr_scored$sentiment[j]] <- nr_scored$value[j]
          }
        } 
        nr_out[[i]] <- nr_df
    }
    nr_out <- bind_rows(nr_out)
    nr_out$sentiment <- out
    if(valence_only){
      return(out)
    } else {
      return(nr_out)
    }
}
```

```{r}
# Read in stopwords
catalan_spanish_stopwords <- 
  c(
    readLines('stopwords/stopwords-ca.txt'),
    readLines('stopwords/stopwords-es.txt')
  )
# Add a few
catalan_spanish_stopwords <- 
  c(catalan_spanish_stopwords,
    c('señor',
      'señoría',
      'señorías',
      'sánchez'))
# Remove numbers
numbers <- as.character(c(0:154,156:1935,1937:1977, 1979:2018))
catalan_spanish_stopwords <- c(catalan_spanish_stopwords, numbers)

file_name <- 'processed_transcript.RData'
if(!file_name %in% dir()){
    # Read in transcripts from session de control
  transcript <- read_csv('data/transcript.csv') %>%
    dplyr::select(date, source, person, text, qa) %>%
    # Remove qa
    filter(!qa) %>%
    # Keep only relevant speakers
    filter(person %in% c('Pedro Sánchez',
                  'Pablo Casado',
                  'Albert Rivera',
                  'Pablo Iglesias',
                  'Joan Tardà',
                  'Carles Campuzano')) %>%
    # remove certain terms
    mutate(text = gsub('(Aplausos)', '', text, fixed = TRUE)) %>%
    mutate(text = gsub('(Rumores)', '', text, fixed = TRUE)) %>%
    mutate(text = gsub('(Risas)', '', text, fixed = TRUE)) %>%
    mutate(text = gsub('(Protestas)', '', text, fixed = TRUE))
  
  # Get by sentence
  sentencify <- function(transcript){
    # Define whether there was an interruption
    transcript$interruption <- FALSE
    for(i in 2:nrow(transcript)){
      if(transcript$person[i] != transcript$person[i-1]){
        transcript$interruption[i] <- TRUE
      }
    }
    # Use interruptions to get intervention number
    transcript$intervention_number <- NA
    counter <- 1
    for(i in 1:nrow(transcript)){
      if(transcript$interruption[i]){
        counter <- counter + 1
      }
      transcript$intervention_number[i] <- counter
    }
    out_list <- list()
    for(i in 1:nrow(transcript)){
      sub_transcript <- transcript[i,]
      # split by sentence 
      sub_transcript_split <- unlist(strsplit(sub_transcript$text, split = '.', fixed = TRUE))
      sub_transcript_split <- trimws(sub_transcript_split)
      # If greater than 1, larger dataframe
      if(length(sub_transcript_split) > 1){
        out <- data_frame(date = sub_transcript$date,
                          source = sub_transcript$source,
                          person = sub_transcript$person,
                          text = sub_transcript_split,
                          intervention_number = sub_transcript$intervention_number)
      } else {
        out <- sub_transcript
      }
      out_list[[i]] <- out
    }
    out <- bind_rows(out_list)
    # Create a sentence number
    out <- out %>%
      mutate(cs = 1) %>%
      group_by(person) %>%
      mutate(sentence_number = cumsum(cs)) %>%
      ungroup %>%
      dplyr::select(-cs) %>%
      # Create a sentence %
      group_by(person) %>%
      mutate(sentence_percent = sentence_number / max(sentence_number) * 100) %>%
      ungroup
    return(out)
  }
  
  # Make transcript a 1 row per person-sentence df
  transcript <- sentencify(transcript = transcript)
  
  # Score the sentiment
  right <- score_sentiment(transcript$text, language = 'es', valence_only = FALSE)
  transcript <- bind_cols(transcript, right)
  save(transcript, file = file_name)
} else {
  load(file_name)
}


# Cumulative average polarity
ma <- function(arr, n=15){
  res = arr
  for(i in n:length(arr)){
    res[i] = mean(arr[(i-n):i])
  }
  res[1:n] <- mean(res[1:n])
  res
}
transcript <- transcript %>%
  group_by(person, intervention_number) %>%
  mutate(sentiment_cumulative_average = ma(sentiment, 10)) %>%
  ungroup

# Flag words
flag_words <- function(x, 
                       words = c('generalitat', 'catalu', 'govern',
                   'catala', 'torra', 'independe',
                   'secioni', 'separat', 'barcelo', 
                   'carreter')){
  out <- list()
  for(i in 1:length(words)){
    out[[i]] <- grepl(words[i], tolower(x))
  }
  z <- data.frame(out, fix.empty.names = FALSE)
  names(z) <- words
  z <- as.matrix(z)
  z <- apply(z, 1, function(x){any(x)})
  return(z)
}

# Define word groups
catalan_words <- c('generalitat', 'catalu',
                   'catala', 'torra', 'independe', 'puigdemont',
                   'secioni', 'separat', 'barcelo')
violence_words <- 
  c('guerra', 'violen', 'balas', 'bala ', 'odio', 'sufrir', 'golp', 'tanqu', 
    'kale borroka', 'batasun', ' atac', ' ataque', 'muerto', 'morir', 'escrach', 'terror',
    'liquidar', 'destruir',
    'rebel', 'víctima', 'balcan')
spain_words <- c('constituc', 'españ')


# Identify catalan sentences in the data
transcript$catalan <- flag_words(transcript$text, words = catalan_words)
transcript$spanish <- flag_words(transcript$text, words = spain_words)
transcript$violence <- flag_words(transcript$text, words = violence_words)

# Combine
combined_df <- transcript %>%
  group_by(person, intervention_number) %>%
  summarise(text = paste0(text, collapse = ' ')) %>%
  ungroup

# Flatten (just one thing per person)
flattened_df <- 
  transcript %>%
  group_by(person) %>%
  summarise(text = paste0(text, collapse = ' ')) %>%
  ungroup


# PAIR WORDS
 # break text into bigrams
bigrams <- transcript %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, into = c("first","second"), sep = " ", remove = FALSE) %>%
  # # remove stop words from tidytext package 
  # anti_join(stop_words, by = c("first" = "word")) %>%
  # anti_join(stop_words, by = c("second" = "word")) %>%
  filter(!first %in% catalan_spanish_stopwords,
         !second %in% catalan_spanish_stopwords) %>%
  filter(str_detect(first, "[a-z]"),
         str_detect(second, "[a-z]")) %>%
  group_by(person) %>%
  count(bigram) %>%
  arrange(-n)

bigram_freqs <- bigrams %>% 
  left_join(bigrams %>% 
              group_by(person) %>% 
              summarise(total = sum(n))) %>%
  mutate(percent = n/total*100) %>%
  group_by(person)

# get the top bigram for each address
top_bigrams <- bigram_freqs %>%
  top_n(10) %>%
  arrange(-percent)

top_bigram_freqs <- bigram_freqs %>%
  semi_join(top_bigrams) %>%
  ungroup() %>%
  arrange(-percent) 
```

## Congreso de los diputados speeches


```{r}
corpus_x <- corpus(simplify_text(flattened_df$text),
                   docnames = flattened_df$person)
docvars(corpus_x, 'Person') <- flattened_df$person
# summary(corpus_x)
# texts(corpus_x)[1]
## Quick search
# kwic(corpus_x, "comandos")
# head(docvars(corpus_x))
## Extract tokens
# tokens(corpus_x, remove_numbers = TRUE,  remove_punct = TRUE)
## Document feature matrix

my_dfm <- dfm(corpus_x,
              remove_punct = TRUE,
              remove = catalan_spanish_stopwords)
# ## See top 20 words
# topfeatures(my_dfm, 5, groups = 'Person')
## Wordcloud
cols <- c(databrew::make_colors(n = length(unique(dim(my_dfm)[1]))))
textplot_wordcloud(my_dfm, min_count = 1, random_order = FALSE,
                   rotation = 0,#0.25, 
                   min_size = 0.65,
                   max_size = 2.8,
                   max_words = 1000,
                   # labelcolor = cols,
                   # labeloffset = 1,
                   labelsize = 1.2,
                   color = cols,
                   comparison = TRUE)

```

On Wednesday, December 12th, Spanish President Pedro Sánchez delivered an address to the Congreso de los Diputados regarding Brexit and the political situation in Catalonia ([official transcription here](http://www.congreso.es/public_oficiales/L12/CONG/DS/PL/DSCD-12-PL-170.PDF)). The speech reflected rising tensions between pro-independence Catalans and the pro-union Sánchez government, and marked a sharp break with Sánchez's previous more conciliatory tone towards Catalonia. The following back-and-forth between Sánchez and the leaders of other major Spanish political parties was tense, and marked by repeated references to violence.

What follows is linguistic analysis of the speeches and counter-speeches of 6 politicians:

- Pedro Sánchez (President, PSOE, unionist)
- Pablo Casado (PP, unionist)
- Albert Rivera (Ciudadanos, unionist)
- Pablo Iglesias (Podemos, ambivalent)
- Carles Campuzano (PDeCat, independentist)
- Joan Tardà (Catalan Left, independentist)

## The questions

1. Are there differences in "polarity" (postivity-negativity) between the different politicians' speeches?

2. Does polarity change when different subjects are discussed (specifically, Catalonia)?

3. Are there differences in complexity between different politicians' speeches?

## The methods

We digitized the speeches from December 12 into a [machine-readable format](https://github.com/joebrew/vilaweb/blob/master/inst/rmd/sesion_de_control/data/transcript.csv), and then used an algorithm based on the [AFINN library](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010) (a dictionary of words with assigned sentimental polarity) to classify each sentence's average emotional direction, excluding the question and answer section at the end. Certain words are categorized as positive or negative, with -5 being the most negative (for example, "bastard", "slut") and +5 being the most positive (for example, "superb" (magnífico) or "thrilled" (encantado)). The majority of words do not have an emotional weight ("to act", "administration", "aquí", etc.) and are classified as 0. The average of a sentence's emotionally-weighted words constitute its positivity.

The below is an example of how the algorithm works on an actual sentence from Pedro Sánchez's speech. The sentence contained some negative words and some positive words, and was classified as neutral.

<table style="width:100%">
  <tr>
    <td><img src="img/sanchez2.png" /></td>
  </tr>
</table>

```{r}
# example <- c("Y lealtad entre administraciones, señorías, algo que por desgracia no se construye con declaraciones que se sitúan fuera de toda lógica y apelan a la violencia, como hemos escuchado en algún dirigente de la Generalitat de Cataluña. Que conste en acta por tanto: tiempo, diálogo y lealtad.")
# example_split <- unlist(strsplit(clean_text(example), ' '))
# 
# x <- score_sentiment(example_split, language = 'es',valence_only = TRUE)
# data.frame(example_split, x)
```

We ran the algorithm on the entire content of speeches, and analyzed trends in positivity. We also tabulated word frequencies and associations. Finally, we ran an analysis on lexical diversity (ie, the complexity of each politicians' speech) in an effort to better understand who their messaging targetted.

## The results

### Are there differences in "polarity" (postivity-negativity) between the different politicians' speeches?

Yes.

Of the 6 speakers examined, 4 had generally "negative" speeches, whereas 2 had "positive" speeches.

```{r}
plot_data <-
  transcript %>%
  # filter(catalan) %>%
  group_by(person) %>%
  summarise(sentiment = mean(sentiment),
            size = n()) %>%
  ungroup %>%
  mutate(person = gsub(' ', '\n', person))

x = ''
y = 'Positivity'
title = 'Overall sentimental polarity in speeches'
subtitle = 'Congreso de los Diputados, 12 de diciembre, 2018'
caption = ''
ggplot(data = plot_data,
       aes(x = person,
           y = sentiment,
           fill = person)) +
  geom_bar(stat = 'identity') +
  theme_vilaweb() +
  scale_fill_manual(name = '', 
                     values = databrew::make_colors(n = length(unique(plot_data$person)))) +
  labs(x = x,
       y = y,
       title = title,
       subtitle = subtitle,
       caption = caption) +
  scale_y_continuous(breaks = seq(-0.4, 0, by = 0.1), labels = c('', 'Negative', '', '', 'Neutral')) +
  geom_text(aes(label = round(sentiment, digits = 2)),
             nudge_y = -0.02,
            alpha = 0.8) +
  theme(legend.position = 'none')
```

Since Sánchez delivered a formal written address, it should come as no surprise that his speech was the most positive (generally formal speeches are more positive than the critiques that follow them). And since and Iglesias' Podemos party is the main supporter of the Sánchez government, it should also came as no surprise that his speech was net positive.

### Does polarity change when different subjects are discussed (specifically, Catalonia)?

Yes. 

The previous chart oversimplifies very large changes in positivity throughout each person's interventions. For example, in the below, we can see wide swings in emotionality. About 1/4 through Sánchez's opening speech, for example, he hit his emotional low point. What was he talking about then? Catalonia.

```{r}
plot_data <- transcript %>%
  mutate(cs = 1) %>%
  mutate(sentence_number_all = cumsum(cs)) 

y_labels <- c('Very negative', 'Negative', 'Neutral', 'Positive', 'Very positive')
x = 'Sentence'
y = 'Positivity'
title = 'Sentimental polarity in speeches'
subtitle = 'Congreso de los Diputados, 12 de diciembre, 2018'
caption = ''
ggplot(data = plot_data,
       aes(x = sentence_number_all,
           y = sentiment,
           color = person,
           group = intervention_number)) +
  geom_line(aes(y = sentiment_cumulative_average),
            size = 0.7) +
  geom_jitter(size = 0.3) +
  # geom_smooth(span = 0.2, 
  #   # n = 200,
  #             se = FALSE,
  #             size = 0.7) +
  scale_color_manual(name = '', 
                     values = databrew::make_colors(n = length(unique(plot_data$person)))) +
  theme_vilaweb() +
  scale_y_continuous(breaks = c(seq(-3, 3, 1.5)), labels = y_labels) + 
  ylim(-3, 3) +
  labs(x = x,
       y = y,
       title = title,
       subtitle = subtitle,
       caption = caption) #+
  # geom_label(data = data.frame(x = 200, 
  #                y = -1,
  #                label = 'Low point'),
  #            aes(x = x,
  #                y = y,
  #                label = label),
  #                group = NA,
  #                color = 'black')
```

In fact, if we filter for only those sentences which contained references to Catalonia*, the emotional polarity values take on a radically different form. If we only examine sentences in which Catalonia is mentioned, we see that all non-independence parties sentimentality becomes more negative, whereas the 2 pro-independence politicians' speech becomes more positive.

```{r}
plot_data <-
  transcript %>%
  filter(catalan) %>%
  group_by(person) %>%
  summarise(sentiment = mean(sentiment),
            size = n(),
            viol = length(which(violence))) %>%
  ungroup %>%
  mutate(p = viol / size * 100) %>%
  mutate(person = gsub(' ', '\n', person))

x = ''
y = 'Positivity'
title = 'Catalonia-specific sentimental polarity in speeches'
subtitle = 'Congreso de los Diputados, 12 de diciembre, 2018'
caption = ''
ggplot(data = plot_data,
       aes(x = person,
           y = sentiment,
           fill = person)) +
  geom_bar(stat = 'identity') +
  theme_vilaweb() +
  scale_fill_manual(name = '', 
                     values = databrew::make_colors(n = length(unique(plot_data$person)))) +
  labs(x = x,
       y = y,
       title = title,
       subtitle = subtitle,
       caption = caption) +
  scale_y_continuous(breaks = seq(-0.4, 0, by = 0.1), labels = c('', 'Negative', '', '', 'Neutral')) +
  geom_text(aes(label = round(sentiment, digits = 2)),
             nudge_y = -0.02,
            alpha = 0.8) +
  theme(legend.position = 'none')
```

The below shows sentimentality over the course of the speeches, filtering only for sentences in which Catalonia is referenced. Note that the large majority of the sentiment curves are below 0 (ie, negative).

```{r}
plot_data <- transcript %>%
  filter(catalan) %>%
  mutate(cs = 1) %>%
  mutate(sentence_number_all = cumsum(cs)) 

y_labels <- c('Very negative', 'Negative', 'Neutral', 'Positive', 'Very positive')
x = 'Sentence'
y = 'Positivity'
title = 'Catalonia-specific sentimental polarity in speeches'
subtitle = 'Congreso de los Diputados, 12 de diciembre, 2018'
caption = ''
ggplot(data = plot_data,
       aes(x = sentence_number_all,
           y = sentiment,
           color = person,
           group = intervention_number)) +
  geom_smooth(span = 1, 
              # n = 5,
              se = FALSE) +
  geom_jitter(size = 0.3) +
  # geom_smooth(span = 0.2, 
  #   # n = 200,
  #             se = FALSE,
  #             size = 0.7) +
  scale_color_manual(name = '', 
                     values = databrew::make_colors(n = length(unique(plot_data$person)))) +
  theme_vilaweb() +
  scale_y_continuous(breaks = c(seq(-3, 3, 1.5)), labels = y_labels) + 
  ylim(-3, 3) +
  labs(x = x,
       y = y,
       title = title,
       subtitle = subtitle,
       caption = caption) +
  geom_hline(yintercept = 0, lty = 2)#+
  # geom_label(data = data.frame(x = 200, 
  #                y = -1,
  #                label = 'Low point'),
  #            aes(x = x,
  #                y = y,
  #                label = label),
  #                group = NA,
  #                color = 'black')
```


#### Are there differences in complexity between different politicians' speeches?  

Lexical diversity is a measure of how many different words are used (ie, how often one repeats words). It is a reflection of how complex or advanced a speech is. For example, children have much lower lexical diversity than adults.  

A speech with high lexical diversity generally correlates with a complicated message. A speech with low lexical diversity among politicians does not generally reflect low intelligence (most politicians are smart), but rather an intentional effort to target a specific audience with simplistic, repetitive messaging. Donald Trump, for example, has become infamous for simple, repetive slogans; his speeches, to no surprise, have very low lexical diversity.

TTR (Type-Token Ratio) is a measure of lexical diversity. Here, it refers to the number of unique words used in any given 100 word sequence. For example, if one repeated the same word 100 times, the TTR would be 1. If one said 100 words and did not repeat at all, the TTR would be 100. The higher the TTR, the higher the level and complexity of speech.

The below chart shows TTR for each politician analyzed. 

```{r}
fn <- 'ttr.RData'
if(fn %in% dir()){
  load(fn)
} else {
  x <- flattened_df
  x$text <- clean_text(x$text)
  results <- data.frame(person = x$person)
  counter <- 0
  out_list <- list()
  for(i in 1:nrow(results)){
    this_person <- results$person[i]
    this_text <- x$text[i]
    this_text_parsed <- unlist(strsplit(this_text, ' '))
    # this_text_parsed <- this_text_parsed[!this_text_parsed %in% catalan_spanish_stopwords]
    possible_indices <- 100:length(this_text_parsed) 
    for(j in 1:1000){
      message(this_person, '---', j)
      counter <- counter + 1
      random_index <- sample(possible_indices, 1)
      random_words <- this_text_parsed[(random_index-99):(random_index)]
      out <- data.frame(word = random_words) %>%
        group_by(word) %>% tally %>% nrow
      done <- data.frame(person = this_person,
                         n = out)
      out_list[[counter]] <- done
    }
  }
  out <- bind_rows(out_list)
  save(out, file = fn)
}
out <- out %>%
  mutate(person = gsub(' ', '\n', person))

agg <- out %>%
  group_by(person) %>%
  summarise(avg = mean(n),
            q75 = quantile(n, 0.75),
            q25 = quantile(n, 0.25),
            q50 = median(n)) %>%
  ungroup

x = '' 
y = 'Lexical diversity'
title = 'Lexical diversity (type-token ratio)'
subtitle = 'Speeches and counter-speeches, Congreso de los Diputados, 12 December 2018'
caption = 'Chart by Joe Brew'

# ggplot(data = agg,
#        aes(x = person,
#            y = avg)) +
#   geom_point(size = 2) +
#   geom_linerange(aes(ymin = q25,
#                      ymax = q75)) +
#   theme_vilaweb() +
#   labs(x = x,
#        y = y,
#        title = title,
#        subtitle = subtitle,
#        caption = caption)
cols <- databrew::make_colors(n = length(unique(out$person)))
ggplot(data = out,
       aes(x = person,
           y = n)) +
  geom_jitter(aes(color = person),
              size = 0.3) +
  geom_violin(aes(fill = person,
                  color = person),
              alpha = 0.6) +
  theme_vilaweb() +
    scale_color_manual(name = '', 
                     values = cols) +
  scale_fill_manual(name = '',
                    values = cols) +
  theme_vilaweb() +
  theme(legend.position = 'none') +
  labs(x = x,
       y = y,
       title = title,
       subtitle = subtitle,
       caption = caption) +
  geom_errorbar(data = agg,
                 aes(x = person,
                     ymin = q25,
                     y = avg,
                     ymax = q75)) 

```

The below shows the percentage of 100 word sequences with a very low TTC (below 60). In other words, these are 100 word sequences in which the 40% of the words have already been said.

```{r}
x = '' 
y = 'Percent of samples with low lexical diversity'
title = 'Percent of speech with low lexical diversity (<60 words per 100)'
subtitle = 'Speeches and counter-speeches, Congreso de los Diputados, 12 December 2018'
caption = 'Chart by Joe Brew'
pd <- out %>%
  group_by(person = gsub(' ', '\n', person)) %>%
  summarise(y = length(which(n <= 60)) / n() * 100)

ggplot(data = pd,
       aes(x = person,
           y = y,
           fill = person)) +
  geom_bar(stat = 'identity') +
   scale_fill_manual(name = '',
                    values = cols) +
  theme_vilaweb() +
  theme(legend.position = 'none') +
  labs(x = x,
       y = y,
       title = title,
       subtitle = subtitle,
       caption = caption) 
```

Lexical diversity is lowest among Albert Rivera and Pablo Casado, suggesting a more Trump-like messaging style (ie, targetting a less ophisticated audience and aiming for sound bites).




## Qualitative interpretation 


In this analysis we have seen that (a) much of what is said at the Congreso is negative, (b) negativity is higher when speaking about Catalonia than about other topics, and (c) there are drastically different levels of speech complexity among different politicians.

Much of this emotional negativity is attributible to violence-related words. For example, Albert Rivera used the words golpe (4), guerra (1), muertos (2), terrorismo (1), and violencia (1). Pablo Casado took on a similar tone, saying golpe/golpista (3), violencia (2), but adding more evocative, specific words like batasunización (1), balcanizar (1), and kale borroka (1). The irony of violence vocabulary is that once it is injected into the discourse, even those who deny it still end up talking about it. For example, even Joan Tardà used the words golpe (3) and violencia/violentos (2). 

The below chart shows the rate of violence-related words when discussing Catalonia (left) vs. other matters (right). For most politicians, the rate is highest on the left (ie, when discussing Catalonia). The most drastic differences are among Casado and Rivera. 


```{r}
pd <- transcript %>%
  mutate(#violence = ifelse(violence, 'Violent', 'Not violent'), 
           catalan = ifelse(catalan, 'Catalan', 'Not Catalan'),
                            person) %>%
  group_by(person, catalan) %>%
  summarise(n = length(which(violence)),
            d = n())  %>%
  # group_by(person) %>%
  mutate(p = n / d * 100)
cols <- databrew::make_colors(n = length(unique(pd$person)))



x = ''
y = 'Tasa'
title = 'Frequency of violence-related words: Catalonia vs. non-Catalonia sentences'
subtitle = 'Congreso de los Diputados, 12 December 2018'
caption = 'Joe Brew'

ggplot(data = pd,
       aes(x = catalan,
           y = p,
           color = person,
           group = person)) +
  geom_path() +
  geom_point() +
  facet_wrap(~person) +
  theme_vilaweb() +
  scale_color_manual(name = '',
                    values = cols) +
  theme(legend.position = 'none') +
  labs(x = x,
       y = y,
       title = title,
       subtitle = subtitle,
       caption = caption)
```

Such a high level of talk about violence is clearly not a reflection of reality - there has been no notable increase in violence in recent months, and the much discussed acts of the last weeks in which pro-independence protestors blocked roadways are arguably illegal, but certainly not violent. Rather, the high frequency of violence-related words is an _anticipatory_ violence, creating a mental frame primed to interpret the upcoming protests of December 21 as war-like.

The construction of a mental framework in which Catalonia is at war is equally apparent in the speech data as in the media. Take, for example, the newspaper headlines from Friday December 14:

<table style="width:100%">
  <tr>
    <td><img src="img/elpais.jpg" /></td>
    <td><img src="img/abc.jpg" /></td>
  </tr>
  <tr>
    <td><img src="img/larazon.jpg" /></td>
    <td><img src="img/elmundo.jpg" /></td>
  </tr>
</table>

El País writes about security force increases and includes the line that the CDR (pro-independence protest groups) "llaman a dar batalla" (have called to battle). ABC uses the military words "comandos" and "asaltar" (to assault) to describe next week's planned protests. La Razón takes a similarly military-esque tone with the words "ejército" (army) and "guerrilla". Meanwhile, El Mundo front-pages an interview with former Spanish President Aznar saying that "the intervention in Catalonia should be total and without a time limit". 

Just like in the congressional speeches, the newspapers are not covering real violence (of which there is not), but rather anticipatory violence. This violence, real or perceived, serves to justify both (a) continued imprisonment of political leaders and (b) direct rule over Catalonia from central Spain. It should come as no surprise that those who favor the previous two measures are also the ones most likely to evoke violence in their speeches.



```{r}
# Frequency of words related to cat
people <- sort(unique(transcript$person))
out_list <- list()
for(i in 1:length(people)){
  message(i)
  this_person <- people[i]
  this_transcript <- transcript %>% filter(catalan) %>% filter(person == this_person)
  this_word_df <- make_word_df(this_transcript$text)
  # Remove stopwords
  this_word_df <- this_word_df %>%
    filter(!word %in% catalan_spanish_stopwords)
  flag <- flag_words(this_word_df$word, words = catalan_words) |
    flag_words(this_word_df$word, words = spain_words)
  # this_word_df <- this_word_df[!flag,]
  # Keep only top 5
  # this_word_df <- this_word_df[1:5,]
  out_list[[i]] <- this_word_df %>% mutate(person = this_person)
}
plot_data <- bind_rows(out_list)
# Keep only violence words
flag <- flag_words(x = plot_data$word, words = violence_words)
# plot_data <- plot_data[flag,]
```
